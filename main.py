import os, time, json, hashlib, re, io import requests, feedparser from bs4 import BeautifulSoup from datetime import datetime from PIL import Image, ImageOps # ================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ (Ø§Ø² Railway â†’ Variables) ================== BOT_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip() # ØªÙˆÚ©Ù† Ø±Ø¨Ø§Øª @BotFather CHAT_IDS = [c.strip() for c in os.getenv("CHAT_IDS", "@news_iran_daily").split(",") if c.strip()] MAX_DAILY = int(os.getenv("MAX_POSTS_PER_DAY", "10")) # Ø³Ù‚Ù Ù¾Ø³Øª Ø¯Ø± 24 Ø³Ø§Ø¹Øª POLL_SECONDS = int(os.getenv("POLL_SECONDS", "120")) # Ù‡Ø± Ú†Ù†Ø¯ Ø«Ø§Ù†ÛŒÙ‡ Ú†Ú© Ø´ÙˆØ¯ (2 Ø¯Ù‚ÛŒÙ‚Ù‡) FRESH_MINUTES = int(os.getenv("FRESH_MINUTES", "180")) # Ø®Ø¨Ø±Ù‡Ø§ÛŒ N Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§Ø®ÛŒØ± LOGO_URL = os.getenv("LOGO_URL", "").strip() # Ù„ÛŒÙ†Ú© PNG Ø´ÙØ§Ù Ø¨Ø±Ø§ÛŒ ÙˆØ§ØªØ±Ù…Ø§Ø±Ú© (Ø§Ø®ØªÛŒØ§Ø±ÛŒ) LIGHT_MODE = os.getenv("LIGHT_MODE", "0").strip() == "1" # Ø­Ø§Ù„Øª Ø³Ø¨Ú© (Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ†) USER_AGENT = "Mozilla/5.0 (TelegramNewsBot)" STATE_FILE = "state.json" # Ø¶Ø¯ØªÚ©Ø±Ø§Ø± Ùˆ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ # =========================================================================== # ====================== Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ (Ø¯Ø§Ø®Ù„ÛŒ + Ø®Ø§Ø±Ø¬ÛŒ) ========================= RSS_FEEDS = [ # Ø¯Ø§Ø®Ù„ÛŒ "https://www.isna.ir/rss", "https://www.farsnews.ir/rss", "https://www.khabaronline.ir/rss", "https://www.irna.ir/rss", "https://www.mehrnews.com/rss", # ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ "https://feeds.bbci.co.uk/persian/rss.xml", "https://parsi.euronews.com/rss", "https://rss.dw.com/rdf/rss-fa-all", # Ø®Ø§Ø±Ø¬ÛŒ (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ) "https://feeds.reuters.com/reuters/topNews", "https://feeds.bbci.co.uk/news/world/rss.xml", "https://edition.cnn.com/rss/edition.rss", "https://www.aljazeera.com/xml/rss/all.xml", ] HEADER_PREFIX = "ğŸ“° ØªØ§Ø²Ù‡â€ŒØªØ±ÛŒÙ†â€ŒÙ‡Ø§ â€” @news_iran_daily" # ========================== Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª/Ø­Ø§ÙØ¸Ù‡ ============================ def today_utc(): return datetime.utcnow().strftime("%Y-%m-%d") def load_state(): try: with open(STATE_FILE, "r", encoding="utf-8") as f: st = json.load(f) if "posted" not in st: st["posted"] = [] if "day" not in st: st["day"] = today_utc() if "count" not in st: st["count"] = 0 return st except: return {"posted": [], "day": today_utc(), "count": 0} def save_state(st): # Ø¨ÛŒØ´â€ŒØ§Ø²Ø­Ø¯ Ø¨Ø²Ø±Ú¯ Ù†Ø´ÙˆØ¯ if len(st["posted"]) > 5000: st["posted"] = st["posted"][-2500:] try: with open(STATE_FILE, "w", encoding="utf-8") as f: json.dump(st, f, ensure_ascii=False, indent=2) except Exception as e: print("State save error:", e) def reset_daily_if_needed(st): if st.get("day") != today_utc(): st["day"] = today_utc() st["count"] = 0 save_state(st) # =========================== Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ/ØªØ´Ø®ÛŒØµ ============================ def clean_html(s: str) -> str: return BeautifulSoup(s or "", "html.parser").get_text().strip() def is_persian(s: str) -> bool: return any('\u0600' <= c <= '\u06FF' for c in s or "") def first_sentences(txt: str, max_chars=300, max_sents=2) -> str: """Ø®Ù„Ø§ØµÙ‡Ù” Ø³Ø¨Ú© Ø¯Ø± Ø­Ø§Ù„Øª LIGHT_MODE ÛŒØ§ fallback.""" txt = re.sub(r"\s+", " ", (txt or "").strip()) parts = re.split(r"(?<=[.!ØŸ\?])\s+", txt) out = " ".join(parts[:max_sents]) or txt[:max_chars] if len(out) > max_chars: out = out[:max_chars].rsplit(" ", 1)[0] + "â€¦" return out # ================== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Hugging Face ================== translator_pipe = None summarizer_pipe = None if not LIGHT_MODE: try: from transformers import pipeline import torch device = 0 if hasattr(torch, "cuda") and torch.cuda.is_available() else -1 # ØªØ±Ø¬Ù…Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒâ†’ÙØ§Ø±Ø³ÛŒ translator_pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-fa", device=device) # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø² ÙØ§Ø±Ø³ÛŒ (Ù…Ø¯Ù„ Ø¨Ø²Ø±Ú¯ Ø§Ø³ØªØ› Ø§Ú¯Ø± RAM Ú©Ù… Ø¨ÙˆØ¯ØŒ LIGHT_MODE=1 Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯) summarizer_pipe = pipeline("summarization", model="m3hrdadfi/bert2bert-fa-summary", tokenizer="m3hrdadfi/bert2bert-fa-summary", device=device) print("âœ… HF pipelines loaded.") except Exception as e: print("âš ï¸ HF load error -> switching to LIGHT_MODE. Err:", e) translator_pipe = None summarizer_pipe = None LIGHT_MODE = True def translate_en_to_fa(text: str) -> str: if not text: return text if translator_pipe is None: # Ø­Ø§Ù„Øª Ø³Ø¨Ú©: Ø¨Ø¯ÙˆÙ† ØªØ±Ø¬Ù…Ù‡ (ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ MyMemory Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒ) return text try: return translator_pipe(text, max_length=400)[0]["translation_text"] except Exception as e: print("Translate error:", e) return text def summarize_fa(text: str) -> str: if not text: return text if summarizer_pipe is None: return first_sentences(text, max_chars=260, max_sents=2) try: # Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ù†Ø¯ Ø¨Ù‡ØªØ±Ù‡ Ú©ÙˆØªØ§Ù‡ Ú©Ù†ÛŒÙ… t = text if len(text) < 1200 else text[:1200] return summarizer_pipe(t, max_length=160, min_length=50, do_sample=False)[0]["summary_text"] except Exception as e: print("Summarize error:", e) return first_sentences(text, max_chars=260, max_sents=2) # ============================ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ ============================= CATEGORIES = { "Ø³ÛŒØ§Ø³ÛŒ": ["Ø³ÛŒØ§Ø³Øª","Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª","Ù…Ø¬Ù„Ø³","Ø¯ÙˆÙ„Øª","ÙˆØ²ÛŒØ±","Ø±Ø¦ÛŒØ³â€ŒØ¬Ù…Ù‡ÙˆØ±","Ù¾Ø§Ø±Ù„Ù…Ø§Ù†"], "Ø§Ù‚ØªØµØ§Ø¯ÛŒ": ["Ø§Ù‚ØªØµØ§Ø¯","Ø¨ÙˆØ±Ø³","Ø¯Ù„Ø§Ø±","ØªÙˆØ±Ù…","Ø¨Ø§Ù†Ú©","ØªØ¬Ø§Ø±Øª","Ù†ÙØª","Ø·Ù„Ø§"], "ÙˆØ±Ø²Ø´ÛŒ": ["ÙˆØ±Ø²Ø´","ÙÙˆØªØ¨Ø§Ù„","ÙˆØ§Ù„ÛŒØ¨Ø§Ù„","Ø¨Ø³Ú©ØªØ¨Ø§Ù„","ØªÛŒÙ…","Ø¨Ø§Ø²ÛŒ","Ù…Ø³Ø§Ø¨Ù‚Ù‡","Ø¬Ø§Ù…"], "Ø¹Ù„Ù…ÛŒ": ["Ø¹Ù„Ù…","ÙÙ†Ø§ÙˆØ±ÛŒ","ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ","Ù‡ÙˆØ´","ÙØ¶Ø§","Ù¾Ú˜ÙˆÙ‡Ø´","Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡"], "Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ": ["Ø­ÙˆØ§Ø¯Ø«","Ø¬Ø§Ù…Ø¹Ù‡","Ø³Ù„Ø§Ù…Øª","Ø¢Ù…ÙˆØ²Ø´","Ù…Ø¯Ø±Ø³Ù‡","Ú©Ø±ÙˆÙ†Ø§","Ø§Ù…Ù†ÛŒØª"], "Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„":["Ø¬Ù‡Ø§Ù†","Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„","Ø¢Ù…Ø±ÛŒÚ©Ø§","Ø§Ø±ÙˆÙ¾Ø§","Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„","Ù†Ø§ØªÙˆ"] } def categorize(title_fa: str, summary_fa: str) -> str: text = f"{title_fa} {summary_fa}" for cat, kwds in CATEGORIES.items(): for k in kwds: if k in text: return cat return "Ø¹Ù…ÙˆÙ…ÛŒ" # ============================ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ ØªØµÙˆÛŒØ±/ÙˆØ§ØªØ±Ù…Ø§Ø±Ú© ====================== def fetch_bytes(url: str, timeout=25) -> bytes | None: try: r = requests.get(url, timeout=timeout, headers={"User-Agent": USER_AGENT}) if r.status_code == 200: return r.content except Exception as e: print("Fetch error:", e) return None def find_image(entry) -> str | None: if hasattr(entry, "media_content") and entry.media_content: u = entry.media_content[0].get("url"); if u: return u if hasattr(entry, "media_thumbnail") and entry.media_thumbnail: u = entry.media_thumbnail[0].get("url"); if u: return u if hasattr(entry, "links"): for l in entry.links: t = (l.get("type") or "").lower() if t.startswith("image") or "image" in t or l.get("rel") == "enclosure": href = l.get("href") if href: return href img = getattr(entry, "image", None) if isinstance(img, dict) and img.get("href"): return img["href"] return None def add_watermark(img_bytes: bytes, logo_bytes: bytes, opacity=0.85, scale=0.22, margin=16) -> bytes: try: from PIL import Image base = Image.open(io.BytesIO(img_bytes)).convert("RGBA") logo = Image.open(io.BytesIO(logo_bytes)).convert("RGBA") new_w = int(base.width * scale) ratio = new_w / logo.width logo = logo.resize((new_w, int(logo.height * ratio))) # Ø´ÙØ§ÙÛŒØª if opacity < 1.0: alpha = logo.split()[-1] # Ø³Ø§Ø¯Ù‡: ØªØºÛŒÛŒØ± Ø¢Ù„ÙØ§ alpha = alpha.point(lambda p: int(p * opacity)) logo.putalpha(alpha) x = base.width - logo.width - margin y = base.height - logo.height - margin canvas = base.copy() canvas.alpha_composite(logo, dest=(x, y)) out = io.BytesIO() canvas.convert("RGB").save(out, format="JPEG", quality=90) return out.getvalue() except Exception as e: print("Watermark error:", e) return img_bytes # ============================== Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… ============================= def make_hash(title: str, link: str) -> str: return hashlib.sha256(f"{title}|{link}".encode("utf-8")).hexdigest() def build_caption(title_fa: str, summary_fa: str, link: str, category: str) -> str: def esc(s): return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;") t = esc(title_fa); s = esc(summary_fa); l = esc(link); c = esc(category.replace(" ","_")) parts = [ HEADER_PREFIX, f"ğŸ·ï¸ #{c}", f"ğŸ—ï¸ <b>{t}</b>", f"ğŸ“ {s}", f"ğŸ”— Ù…Ù†Ø¨Ø¹: <a href=\"{l}\">{l}</a>" if l else "" ] return "\n".join([p for p in parts if p]).strip() def send_to_channels(caption_html: str, image_url: str | None): base = f"https://api.telegram.org/bot{BOT_TOKEN}" logo_bytes = fetch_bytes(LOGO_URL) if LOGO_URL else None for ch in CHAT_IDS: try: if image_url: photo_raw = fetch_bytes(image_url) if photo_raw and logo_bytes: photo_raw = add_watermark(photo_raw, logo_bytes) if photo_raw: files = {"photo": ("image.jpg", photo_raw, "image/jpeg")} data = {"chat_id": ch, "caption": caption_html, "parse_mode": "HTML", "disable_web_page_preview": False} r = requests.post(f"{base}/sendPhoto", data=data, files=files, timeout=30) else: r = requests.post(f"{base}/sendMessage", data={"chat_id": ch, "text": caption_html, "parse_mode": "HTML"}, timeout=30) else: r = requests.post(f"{base}/sendMessage", data={"chat_id": ch, "text": caption_html, "parse_mode": "HTML"}, timeout=30) print(f"[{ch}] {r.status_code} {r.text[:160]}") time.sleep(1.1) except Exception as e: print(f"Send error for {ch}:", e) # ============================== Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø±Ø¨Ø§Øª ============================== def entry_datetime(entry): dt = None if hasattr(entry, "published_parsed") and entry.published_parsed: dt = datetime(*entry.published_parsed[:6]) elif hasattr(entry, "updated_parsed") and entry.updated_parsed: dt = datetime(*entry.updated_parsed[:6]) return dt def process_entry(entry, state) -> bool: title = clean_html(getattr(entry, "title", "") or "") link = getattr(entry, "link", "") or "" summ = clean_html(getattr(entry, "summary", "") or "") if not title and not link: return False # ØªØ§Ø²Ú¯ÛŒ Ø®Ø¨Ø± (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡) dt = entry_datetime(entry) if dt: age_min = (datetime.utcnow() - dt).total_seconds() / 60.0 if age_min > FRESH_MINUTES: return False h = make_hash(title, link) if h in state["posted"]: return False # ØªÚ©Ø±Ø§Ø±ÛŒ body = summ or title # ØªØ±Ø¬Ù…Ù‡ + Ø®Ù„Ø§ØµÙ‡ if is_persian(title + " " + body): title_fa = title summary_fa = summarize_fa(body) else: title_fa = translate_en_to_fa(title) if title else title summary_fa = translate_en_to_fa(body) summary_fa = summarize_fa(summary_fa) # ØªÛŒØªØ± ØµÙ…ÛŒÙ…ÛŒ + Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø³Ø§Ø¯Ù‡ key = f"{title_fa} {summary_fa}" if any(w in key for w in ["ÙÙˆØ±ÛŒ","Ù‡Ø´Ø¯Ø§Ø±","Ø§Ù†ÙØ¬Ø§Ø±","Ø²Ù„Ø²Ù„Ù‡","Ø³ÛŒÙ„","Ø¢ØªØ´"]): prefix = "âš¡" elif any(w in key for w in ["Ø§Ù‚ØªØµØ§Ø¯","Ø¯Ù„Ø§Ø±","Ø¨ÙˆØ±Ø³","ØªÙˆØ±Ù…","Ø¨Ø§Ù†Ú©"]): prefix = "ğŸ’°" elif any(w in key for w in ["ÙÙˆØªØ¨Ø§Ù„","ÙˆØ±Ø²Ø´","ØªÛŒÙ…","Ø¨Ø§Ø²ÛŒ","Ù…Ø³Ø§Ø¨Ù‚Ù‡"]): prefix = "âš½" elif any(w in key for w in ["Ø¹Ù„Ù…","ÙÙ†Ø§ÙˆØ±ÛŒ","Ù‡ÙˆØ´","ÙØ¶Ø§","Ù¾Ú˜ÙˆÙ‡Ø´"]): prefix = "ğŸ¤–" else: prefix = "ğŸ—ï¸" title_fa = f"{prefix} {title_fa}" # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ category = categorize(title_fa, summary_fa) # Ú©Ù¾Ø´Ù† caption = build_caption(title_fa, summary_fa, link, category) # ØªØµÙˆÛŒØ± img_url = find_image(entry) # Ø§Ø±Ø³Ø§Ù„ send_to_channels(caption, img_url) # Ø¶Ø¯ØªÚ©Ø±Ø§Ø± + Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ state["posted"].append(h) state["count"] += 1 save_state(state) return True def run_once(state): # Ø±ÛŒØ³Øª Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± 00:00 UTC reset_daily_if_needed(state) if state["count"] >= MAX_DAILY: print(f"â¸ï¸ Ø³Ù‚Ù Ø±ÙˆØ²Ø§Ù†Ù‡ Ù¾Ø± Ø´Ø¯Ù‡ ({MAX_DAILY}).") return headers = {"User-Agent": USER_AGENT} sent_now = 0 for feed_url in RSS_FEEDS: if state["count"] >= MAX_DAILY: print("â†©ï¸ ØªÙˆÙ‚Ù: Ø³Ù‚Ù Ø±ÙˆØ²Ø§Ù†Ù‡.") return try: d = feedparser.parse(feed_url, request_headers=headers) for entry in d.entries[:12]: if state["count"] >= MAX_DAILY: print("â†©ï¸ ØªÙˆÙ‚Ù: Ø³Ù‚Ù Ø±ÙˆØ²Ø§Ù†Ù‡.") return ok = process_entry(entry, state) if ok: sent_now += 1 print("âœ… Ù¾Ø³Øª Ø´Ø¯:", getattr(entry, "title", "")[:80]) time.sleep(2) except Exception as e: print("Feed error:", feed_url, e) if sent_now == 0: print("â„¹ï¸ Ø®Ø¨Ø± ØªØ§Ø²Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ ØªØ¹ÛŒÛŒÙ†â€ŒØ´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.") def main(): if not BOT_TOKEN: print("âŒ TELEGRAM_TOKEN ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡.") time.sleep(10); return if not CHAT_IDS: print("âŒ CHAT_IDS Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.") time.sleep(10); return state = load_state() while True: try: run_once(state) except Exception as e: print("Run error:", e) time.sleep(POLL_SECONDS) if __name__ == "__main__": main() 
