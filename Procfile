import os, time, json, hashlib, re, io import requests, feedparser from bs4 import BeautifulSoup from datetime import datetime from PIL import Image, ImageOps # ================== تنظیمات اصلی (از Railway → Variables) ================== BOT_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip() # توکن ربات @BotFather CHAT_IDS = [c.strip() for c in os.getenv("CHAT_IDS", "@news_iran_daily").split(",") if c.strip()] MAX_DAILY = int(os.getenv("MAX_POSTS_PER_DAY", "10")) # سقف پست در 24 ساعت POLL_SECONDS = int(os.getenv("POLL_SECONDS", "120")) # هر چند ثانیه چک شود (2 دقیقه) FRESH_MINUTES = int(os.getenv("FRESH_MINUTES", "180")) # خبرهای N دقیقه اخیر LOGO_URL = os.getenv("LOGO_URL", "").strip() # لینک PNG شفاف برای واترمارک (اختیاری) LIGHT_MODE = os.getenv("LIGHT_MODE", "0").strip() == "1" # حالت سبک (بدون مدل‌های سنگین) USER_AGENT = "Mozilla/5.0 (TelegramNewsBot)" STATE_FILE = "state.json" # ضدتکرار و شمارنده روزانه # =========================================================================== # ====================== منابع خبری (داخلی + خارجی) ========================= RSS_FEEDS = [ # داخلی "https://www.isna.ir/rss", "https://www.farsnews.ir/rss", "https://www.khabaronline.ir/rss", "https://www.irna.ir/rss", "https://www.mehrnews.com/rss", # فارسی بین‌الملل "https://feeds.bbci.co.uk/persian/rss.xml", "https://parsi.euronews.com/rss", "https://rss.dw.com/rdf/rss-fa-all", # خارجی (انگلیسی) "https://feeds.reuters.com/reuters/topNews", "https://feeds.bbci.co.uk/news/world/rss.xml", "https://edition.cnn.com/rss/edition.rss", "https://www.aljazeera.com/xml/rss/all.xml", ] HEADER_PREFIX = "📰 تازه‌ترین‌ها — @news_iran_daily" # ========================== ابزارهای وضعیت/حافظه ============================ def today_utc(): return datetime.utcnow().strftime("%Y-%m-%d") def load_state(): try: with open(STATE_FILE, "r", encoding="utf-8") as f: st = json.load(f) if "posted" not in st: st["posted"] = [] if "day" not in st: st["day"] = today_utc() if "count" not in st: st["count"] = 0 return st except: return {"posted": [], "day": today_utc(), "count": 0} def save_state(st): # بیش‌ازحد بزرگ نشود if len(st["posted"]) > 5000: st["posted"] = st["posted"][-2500:] try: with open(STATE_FILE, "w", encoding="utf-8") as f: json.dump(st, f, ensure_ascii=False, indent=2) except Exception as e: print("State save error:", e) def reset_daily_if_needed(st): if st.get("day") != today_utc(): st["day"] = today_utc() st["count"] = 0 save_state(st) # =========================== ابزارهای متنی/تشخیص ============================ def clean_html(s: str) -> str: return BeautifulSoup(s or "", "html.parser").get_text().strip() def is_persian(s: str) -> bool: return any('\u0600' <= c <= '\u06FF' for c in s or "") def first_sentences(txt: str, max_chars=300, max_sents=2) -> str: """خلاصهٔ سبک در حالت LIGHT_MODE یا fallback.""" txt = re.sub(r"\s+", " ", (txt or "").strip()) parts = re.split(r"(?<=[.!؟\?])\s+", txt) out = " ".join(parts[:max_sents]) or txt[:max_chars] if len(out) > max_chars: out = out[:max_chars].rsplit(" ", 1)[0] + "…" return out # ================== بارگذاری مدل‌های رایگان Hugging Face ================== translator_pipe = None summarizer_pipe = None if not LIGHT_MODE: try: from transformers import pipeline import torch device = 0 if hasattr(torch, "cuda") and torch.cuda.is_available() else -1 # ترجمه انگلیسی→فارسی translator_pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-fa", device=device) # خلاصه‌ساز فارسی (مدل بزرگ است؛ اگر RAM کم بود، LIGHT_MODE=1 بگذارید) summarizer_pipe = pipeline("summarization", model="m3hrdadfi/bert2bert-fa-summary", tokenizer="m3hrdadfi/bert2bert-fa-summary", device=device) print("✅ HF pipelines loaded.") except Exception as e: print("⚠️ HF load error -> switching to LIGHT_MODE. Err:", e) translator_pipe = None summarizer_pipe = None LIGHT_MODE = True def translate_en_to_fa(text: str) -> str: if not text: return text if translator_pipe is None: # حالت سبک: بدون ترجمه (یا می‌تونی MyMemory رایگان اضافه کنی) return text try: return translator_pipe(text, max_length=400)[0]["translation_text"] except Exception as e: print("Translate error:", e) return text def summarize_fa(text: str) -> str: if not text: return text if summarizer_pipe is None: return first_sentences(text, max_chars=260, max_sents=2) try: # برای متن‌های بلند بهتره کوتاه کنیم t = text if len(text) < 1200 else text[:1200] return summarizer_pipe(t, max_length=160, min_length=50, do_sample=False)[0]["summary_text"] except Exception as e: print("Summarize error:", e) return first_sentences(text, max_chars=260, max_sents=2) # ============================ دسته‌بندی موضوعی ============================= CATEGORIES = { "سیاسی": ["سیاست","انتخابات","مجلس","دولت","وزیر","رئیس‌جمهور","پارلمان"], "اقتصادی": ["اقتصاد","بورس","دلار","تورم","بانک","تجارت","نفت","طلا"], "ورزشی": ["ورزش","فوتبال","والیبال","بسکتبال","تیم","بازی","مسابقه","جام"], "علمی": ["علم","فناوری","تکنولوژی","هوش","فضا","پژوهش","دانشگاه"], "اجتماعی": ["حوادث","جامعه","سلامت","آموزش","مدرسه","کرونا","امنیت"], "بین‌الملل":["جهان","بین‌الملل","آمریکا","اروپا","سازمان ملل","ناتو"] } def categorize(title_fa: str, summary_fa: str) -> str: text = f"{title_fa} {summary_fa}" for cat, kwds in CATEGORIES.items(): for k in kwds: if k in text: return cat return "عمومی" # ============================ ابزارهای تصویر/واترمارک ====================== def fetch_bytes(url: str, timeout=25) -> bytes | None: try: r = requests.get(url, timeout=timeout, headers={"User-Agent": USER_AGENT}) if r.status_code == 200: return r.content except Exception as e: print("Fetch error:", e) return None def find_image(entry) -> str | None: if hasattr(entry, "media_content") and entry.media_content: u = entry.media_content[0].get("url"); if u: return u if hasattr(entry, "media_thumbnail") and entry.media_thumbnail: u = entry.media_thumbnail[0].get("url"); if u: return u if hasattr(entry, "links"): for l in entry.links: t = (l.get("type") or "").lower() if t.startswith("image") or "image" in t or l.get("rel") == "enclosure": href = l.get("href") if href: return href img = getattr(entry, "image", None) if isinstance(img, dict) and img.get("href"): return img["href"] return None def add_watermark(img_bytes: bytes, logo_bytes: bytes, opacity=0.85, scale=0.22, margin=16) -> bytes: try: from PIL import Image base = Image.open(io.BytesIO(img_bytes)).convert("RGBA") logo = Image.open(io.BytesIO(logo_bytes)).convert("RGBA") new_w = int(base.width * scale) ratio = new_w / logo.width logo = logo.resize((new_w, int(logo.height * ratio))) # شفافیت if opacity < 1.0: alpha = logo.split()[-1] # ساده: تغییر آلفا alpha = alpha.point(lambda p: int(p * opacity)) logo.putalpha(alpha) x = base.width - logo.width - margin y = base.height - logo.height - margin canvas = base.copy() canvas.alpha_composite(logo, dest=(x, y)) out = io.BytesIO() canvas.convert("RGB").save(out, format="JPEG", quality=90) return out.getvalue() except Exception as e: print("Watermark error:", e) return img_bytes # ============================== ارسال به تلگرام ============================= def make_hash(title: str, link: str) -> str: return hashlib.sha256(f"{title}|{link}".encode("utf-8")).hexdigest() def build_caption(title_fa: str, summary_fa: str, link: str, category: str) -> str: def esc(s): return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;") t = esc(title_fa); s = esc(summary_fa); l = esc(link); c = esc(category.replace(" ","_")) parts = [ HEADER_PREFIX, f"🏷️ #{c}", f"🗞️ <b>{t}</b>", f"📝 {s}", f"🔗 منبع: <a href=\"{l}\">{l}</a>" if l else "" ] return "\n".join([p for p in parts if p]).strip() def send_to_channels(caption_html: str, image_url: str | None): base = f"https://api.telegram.org/bot{BOT_TOKEN}" logo_bytes = fetch_bytes(LOGO_URL) if LOGO_URL else None for ch in CHAT_IDS: try: if image_url: photo_raw = fetch_bytes(image_url) if photo_raw and logo_bytes: photo_raw = add_watermark(photo_raw, logo_bytes) if photo_raw: files = {"photo": ("image.jpg", photo_raw, "image/jpeg")} data = {"chat_id": ch, "caption": caption_html, "parse_mode": "HTML", "disable_web_page_preview": False} r = requests.post(f"{base}/sendPhoto", data=data, files=files, timeout=30) else: r = requests.post(f"{base}/sendMessage", data={"chat_id": ch, "text": caption_html, "parse_mode": "HTML"}, timeout=30) else: r = requests.post(f"{base}/sendMessage", data={"chat_id": ch, "text": caption_html, "parse_mode": "HTML"}, timeout=30) print(f"[{ch}] {r.status_code} {r.text[:160]}") time.sleep(1.1) except Exception as e: print(f"Send error for {ch}:", e) # ============================== منطق اصلی ربات ============================== def entry_datetime(entry): dt = None if hasattr(entry, "published_parsed") and entry.published_parsed: dt = datetime(*entry.published_parsed[:6]) elif hasattr(entry, "updated_parsed") and entry.updated_parsed: dt = datetime(*entry.updated_parsed[:6]) return dt def process_entry(entry, state) -> bool: title = clean_html(getattr(entry, "title", "") or "") link = getattr(entry, "link", "") or "" summ = clean_html(getattr(entry, "summary", "") or "") if not title and not link: return False # تازگی خبر (برای جلوگیری از سیل اولیه) dt = entry_datetime(entry) if dt: age_min = (datetime.utcnow() - dt).total_seconds() / 60.0 if age_min > FRESH_MINUTES: return False h = make_hash(title, link) if h in state["posted"]: return False # تکراری body = summ or title # ترجمه + خلاصه if is_persian(title + " " + body): title_fa = title summary_fa = summarize_fa(body) else: title_fa = translate_en_to_fa(title) if title else title summary_fa = translate_en_to_fa(body) summary_fa = summarize_fa(summary_fa) # تیتر صمیمی + ایموجی ساده key = f"{title_fa} {summary_fa}" if any(w in key for w in ["فوری","هشدار","انفجار","زلزله","سیل","آتش"]): prefix = "⚡" elif any(w in key for w in ["اقتصاد","دلار","بورس","تورم","بانک"]): prefix = "💰" elif any(w in key for w in ["فوتبال","ورزش","تیم","بازی","مسابقه"]): prefix = "⚽" elif any(w in key for w in ["علم","فناوری","هوش","فضا","پژوهش"]): prefix = "🤖" else: prefix = "🗞️" title_fa = f"{prefix} {title_fa}" # دسته‌بندی category = categorize(title_fa, summary_fa) # کپشن caption = build_caption(title_fa, summary_fa, link, category) # تصویر img_url = find_image(entry) # ارسال send_to_channels(caption, img_url) # ضدتکرار + شمارنده state["posted"].append(h) state["count"] += 1 save_state(state) return True def run_once(state): # ریست شمارنده روزانه در 00:00 UTC reset_daily_if_needed(state) if state["count"] >= MAX_DAILY: print(f"⏸️ سقف روزانه پر شده ({MAX_DAILY}).") return headers = {"User-Agent": USER_AGENT} sent_now = 0 for feed_url in RSS_FEEDS: if state["count"] >= MAX_DAILY: print("↩️ توقف: سقف روزانه.") return try: d = feedparser.parse(feed_url, request_headers=headers) for entry in d.entries[:12]: if state["count"] >= MAX_DAILY: print("↩️ توقف: سقف روزانه.") return ok = process_entry(entry, state) if ok: sent_now += 1 print("✅ پست شد:", getattr(entry, "title", "")[:80]) time.sleep(2) except Exception as e: print("Feed error:", feed_url, e) if sent_now == 0: print("ℹ️ خبر تازه‌ای در بازه تعیین‌شده پیدا نشد.") def main(): if not BOT_TOKEN: print("❌ TELEGRAM_TOKEN تنظیم نشده.") time.sleep(10); return if not CHAT_IDS: print("❌ CHAT_IDS خالی است.") time.sleep(10); return state = load_state() while True: try: run_once(state) except Exception as e: print("Run error:", e) time.sleep(POLL_SECONDS) if __name__ == "__main__": main() 
